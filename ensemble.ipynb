{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRMwRwdc_6Qb",
    "outputId": "8408f85a-6675-448e-986a-a983fd9810b3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pTq8JWaGhQ1-",
    "outputId": "a06f15f7-fab0-4d0d-b101-0cb9e523e2e5"
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGUkQl0jhzHu"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from datasets import Dataset as HFDataset\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mG0ub1MbhH-q"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "class HuggingFaceTransformer:\n",
    "    def __init__(self, model_name='roberta-base'):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self, texts, labels, epochs=3, batch_size=8):\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "        train_dataset = HFDataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "        val_dataset = HFDataset.from_dict({'text': val_texts, 'label': val_labels})\n",
    "\n",
    "        def tokenize(example):\n",
    "            return self.tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "        train_dataset = train_dataset.map(tokenize, batched=True, num_proc=1)\n",
    "        val_dataset = val_dataset.map(tokenize, batched=True, num_proc=1)\n",
    "\n",
    "        train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "        val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results_hf',\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs_hf',\n",
    "            logging_steps=10,\n",
    "            save_total_limit=1,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            preds = logits.argmax(-1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        trainer.train()\n",
    "        self.save(\"models/hf_roberta\")\n",
    "\n",
    "    def predict(self, texts, batch_size=16):\n",
    "      self.model.eval()\n",
    "      all_preds = []\n",
    "      for i in range(0, len(texts), batch_size):\n",
    "          batch_texts = texts[i:i + batch_size]\n",
    "          inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(self.device)\n",
    "          with torch.no_grad():\n",
    "              outputs = self.model(**inputs)\n",
    "              probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "              batch_preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "          all_preds.extend(batch_preds)\n",
    "      return all_preds\n",
    "\n",
    "\n",
    "    def predict_proba(self, texts, batch_size=16):\n",
    "        self.model.eval()\n",
    "        all_probs = []\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                batch_probs = probs[:, 1].cpu().numpy()\n",
    "\n",
    "            all_probs.extend(batch_probs)\n",
    "\n",
    "        return np.array(all_probs)\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(path, local_files_only=True)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "        instance = cls()\n",
    "        instance.tokenizer = tokenizer\n",
    "        instance.model = model.to(instance.device)\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUDlT2YkCpLR"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import spacy\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class StylometricFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def get_lexical_features(self, doc) -> Dict[str, float]:\n",
    "        words = [token.text.lower() for token in doc if not token.is_punct]\n",
    "        word_lengths = [len(word) for word in words]\n",
    "        unique_words = set(words)\n",
    "        total_words = len(words)\n",
    "\n",
    "        function_words = [token.text.lower() for token in doc if token.is_stop]\n",
    "        content_words = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "        return {\n",
    "            'avg_word_length': np.mean(word_lengths) if word_lengths else 0,\n",
    "            'type_token_ratio': len(unique_words) / total_words if total_words else 0,\n",
    "            'function_word_ratio': len(function_words) / total_words if total_words else 0,\n",
    "            'content_word_ratio': len(content_words) / total_words if total_words else 0,\n",
    "        }\n",
    "\n",
    "    def get_syntactic_features(self, doc) -> Dict[str, float]:\n",
    "        total_tokens = len(doc)\n",
    "        pos_counts = Counter([token.pos_ for token in doc])\n",
    "\n",
    "        # Limit to common POS tags\n",
    "        common_tags = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON']\n",
    "        pos_features = {\n",
    "            f'pos_{pos}': pos_counts.get(pos, 0) / total_tokens if total_tokens else 0\n",
    "            for pos in common_tags\n",
    "        }\n",
    "\n",
    "        clause_count = sum(1 for _ in doc.sents)\n",
    "\n",
    "        return {\n",
    "            **pos_features,\n",
    "            'clause_count': clause_count\n",
    "        }\n",
    "\n",
    "    def get_statistical_features(self, doc) -> Dict[str, float]:\n",
    "        sentences = list(doc.sents)\n",
    "        text = doc.text\n",
    "        words = text.split()\n",
    "\n",
    "        sent_lengths = [len(sent.text.split()) for sent in sentences]\n",
    "        capitalized_words = sum(1 for word in words if word and word[0].isupper())\n",
    "\n",
    "        return {\n",
    "            'avg_sentence_length': np.mean(sent_lengths) if sent_lengths else 0,\n",
    "            'exclamation_ratio': text.count('!') / len(text) if text else 0,\n",
    "            'question_ratio': text.count('?') / len(text) if text else 0,\n",
    "            'capitalized_ratio': capitalized_words / len(words) if words else 0\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts: List[str]) -> np.ndarray:\n",
    "        features_list = []\n",
    "\n",
    "        for doc in tqdm(self.nlp.pipe(texts, batch_size=50), total=len(texts), desc=\"Extracting features\"):\n",
    "            features = {}\n",
    "            features.update(self.get_lexical_features(doc))\n",
    "            features.update(self.get_syntactic_features(doc))\n",
    "            features.update(self.get_statistical_features(doc))\n",
    "\n",
    "            feature_values = [value for key, value in sorted(features.items())]\n",
    "            features_list.append(feature_values)\n",
    "\n",
    "        return np.array(features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYfXaJGOCNSm"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "class StylometricClassifier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the stylometric classifier using logistic regression\n",
    "        \"\"\"\n",
    "        self.classifier = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            # Add L2 regularization to prevent overfitting\n",
    "            C=1.0,\n",
    "            # Use balanced class weights for potentially imbalanced datasets\n",
    "            class_weight='balanced',\n",
    "            verbose=1\n",
    "        )\n",
    "        self.feature_extractor = StylometricFeatureExtractor()\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, texts, labels):\n",
    "        \"\"\"\n",
    "        Extract features and train the classifier\n",
    "        \"\"\"\n",
    "        # Extract and scale features\n",
    "        X = self.feature_extractor.transform(texts)\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        # Train classifier\n",
    "        self.classifier.fit(X_scaled, labels)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, texts):\n",
    "        \"\"\"\n",
    "        Get probability predictions\n",
    "        \"\"\"\n",
    "        # Extract and scale features\n",
    "        X = self.feature_extractor.transform(texts)\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "\n",
    "        # Get predictions\n",
    "        return self.classifier.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "    def predict(self, texts):\n",
    "        \"\"\"\n",
    "        Get binary predictions\n",
    "        \"\"\"\n",
    "        return (self.predict_proba(texts) >= 0.5).astype(int)\n",
    "\n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Get feature importance scores from the logistic regression coefficients\n",
    "        \"\"\"\n",
    "        if not hasattr(self.classifier, 'coef_'):\n",
    "            raise ValueError(\"Model has not been trained yet\")\n",
    "\n",
    "        # Get feature names from a sample transformation\n",
    "        sample_text = [\"Sample text for feature names\"]\n",
    "        X = self.feature_extractor.transform(sample_text)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Create feature importance dictionary\n",
    "        importance = np.abs(self.classifier.coef_[0])\n",
    "        feature_importance = {\n",
    "            f\"feature_{i}\": importance[i]\n",
    "            for i in range(n_features)\n",
    "        }\n",
    "\n",
    "        # Sort by absolute importance\n",
    "        return dict(sorted(\n",
    "            feature_importance.items(),\n",
    "            key=lambda x: abs(x[1]),\n",
    "            reverse=True\n",
    "        ))\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save model components\n",
    "        \"\"\"\n",
    "        joblib.dump({\n",
    "            'classifier': self.classifier,\n",
    "            'scaler': self.scaler\n",
    "        }, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"\n",
    "        Load saved model\n",
    "        \"\"\"\n",
    "        components = joblib.load(path)\n",
    "        model = cls()\n",
    "        model.classifier = components['classifier']\n",
    "        model.scaler = components['scaler']\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-pXQ8LjAfBH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zBEmGc4mSfM"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/skgabriel/cs162-final-dev/refs/heads/main/german_wikipedia.jsonl\"\n",
    "dev_df = pd.read_json(url, lines=True)\n",
    "human_texts = dev_df['human_text'].tolist()\n",
    "machine_texts = dev_df['machine_text'].tolist()\n",
    "\n",
    "texts = human_texts + machine_texts\n",
    "labels = [0] * len(human_texts) + [1] * len(machine_texts)\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/skgabriel/cs162-final-dev/refs/heads/main/toefl.json\"\n",
    "# dev_df = pd.read_json(url)\n",
    "# human_texts = dev_df['document'].tolist()\n",
    "# machine_texts = []\n",
    "\n",
    "# texts = human_texts\n",
    "# labels = [0] * len(human_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437,
     "referenced_widgets": [
      "df3c4fdd92da4c609c05af34aa1197a6",
      "fa2216384f4147d6891d803145e890d9",
      "7d8fff10a43a4eac8b45bd765b5d20c0",
      "53e798158db14200b2634b767dac1a9e",
      "05211849168b4e85a7820a31120f6b7f",
      "0306503af5b248d4bd8caa53166a90d3",
      "d423b3cb88f44672bc99fe274f3038e5",
      "db6e594efb41466aab879a262a76d1df",
      "c64e327022044cc0a83c85c8c0c81307",
      "150373a0cd5549e8b351333d9faaa4b1",
      "6011739e01cd414784306229f356aa37",
      "0a156f7181fc4837b6056d1d9c313399",
      "2135bfd4f2984e179206d57c9a8de1e9",
      "43cfd61ea1264594bd183f238838b3ce",
      "4bd4316b577745ceaf6726b17ff30b7f",
      "de4178e1e91a4b05adfdab27c380cfed",
      "3abe4ea507a84599b9d530905250905c",
      "b982923b521f451685cf832df29d3eed",
      "525d7851c52e47caa2f0ab1192e03f37",
      "c1ca864061b34440bda228a7d9f6a9b9",
      "6b38f11621bd482aace39681f0ff41e1",
      "655404650bda4ab896433d9411051157",
      "e986b1d674d94922a8ebdad8af2fc878",
      "d6b20101a6764c83bfd3fd67331a5b2e",
      "222db012a86c4ebe97f2686f8f8fd7f7",
      "0dd2f478314f4353a33d4d191adf8b8d",
      "2f5d57d7863d48879987b69efa5283e5",
      "b2f1fc152ecd4643ac2c2ca75341965a",
      "fa3bd2e4e1964c5ab6e44b14e5dcab2e",
      "dc9f32f45e874e5c9d8deddced10f06b",
      "7439de19fd1441ca83cd3c774bb8362f",
      "3560774c7d6c4b0bb274d600f0176a70",
      "b820ef91e9094ddc8b57b58c188b1f47",
      "fbef4e90c5e040669e41dea2095b5102",
      "ffb7199b00a2485e912ab7d8b743193e",
      "3aedab2315014422815e6bbbbdceabbf",
      "00ebb3e0dd894076b0754acf83665057",
      "f14032c384794954b8e4bc9514c11330",
      "fb71c85cd87c4299974771f47f4d9dce",
      "5bff7bc50fcd4aa2b9801728780816c5",
      "aa33f506cd5647c58bcd7cb86d06bf9c",
      "087f464afde1413fbaa4978aebf243d2",
      "376cb8e2f2be45b480e49c3ca40b2f8f",
      "a68dffbbf3564354bc37029e15db14cf",
      "0c2ff50f0d6e402bae14ac0297e9d169",
      "e906ae66a7984ad7b56562f1728ab60e",
      "cb04e8ae64ca4364822ef7bc1208d383",
      "8e080caa9cae44b3a725aba9bff0cc2a",
      "2dfc0c9bda324e99a758786dcb5ade64",
      "bbcf77397d4c426b8cdb491a8d5128c1",
      "a3307b37e24640489e4d8984323d7b01",
      "d63417269f1b4ce0b412214d170496a5",
      "5dcd43561d5b4e1a819fc2378646a894",
      "a0f2512f79e341b59baf1bcc15319b97",
      "d687e2aa62ed4541aa3c9333e15c07f2",
      "3aa5fb076f6747c48ca025e40f9335fa",
      "98419c294c97402eaba14cac29e22344",
      "c163075c2e7949e2b2a8effeadd100f6",
      "629e3ebf31e24dd993b8c8eb72e98c26",
      "23c5711e6bd04b22a88edd496c8ddeb3",
      "76dc06dd2d054fcbb727c8dbd843c549",
      "0475e6e6bdbb44a99d7de4f74295ae2c",
      "5a289ed97bc64cfb9f354dcaf27636ae",
      "ed301087a3944735a04046f846fccb5b",
      "348fb1466f3741cdbcb8d587ded4a116",
      "366c5bbd23ae4bcd9d5be2619a138e1f"
     ]
    },
    "id": "b-lY6E9ztdUu",
    "outputId": "7750c419-5edc-49b4-c894-de4cfd59f86b"
   },
   "outputs": [],
   "source": [
    "roberta_model = HuggingFaceTransformer.load(\"/content/drive/MyDrive/hf_roberta_new\")\n",
    "\n",
    "preds = roberta_model.predict(texts)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision = precision_score(labels, preds, pos_label=1)\n",
    "recall = recall_score(labels, preds, pos_label=1)\n",
    "f1 = f1_score(labels, preds, pos_label=1)\n",
    "\n",
    "print(f\"AI Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"AI Precision: {precision:.3f}\")\n",
    "print(f\"AI Recall:    {recall:.3f}\")\n",
    "print(f\"AI F1 Score:  {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKCdH095BgS7",
    "outputId": "9bcf29dd-407e-415f-9794-f5981c0f0dfe"
   },
   "outputs": [],
   "source": [
    "stylometric_model = StylometricClassifier.load(\"/content/drive/MyDrive/stylometric_model.pkl\")\n",
    "preds = stylometric_model.predict(texts)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision = precision_score(labels, preds, pos_label=1)\n",
    "recall = recall_score(labels, preds, pos_label=1)\n",
    "f1 = f1_score(labels, preds, pos_label=1)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"AI Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"AI Precision: {precision:.3f}\")\n",
    "print(f\"AI Recall:    {recall:.3f}\")\n",
    "print(f\"AI F1 Score:  {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYUXBJwMUG3k"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "480tZwwFp5kP"
   },
   "outputs": [],
   "source": [
    "def predict_ensemble(texts, hf_model, stylometric_model, weight_hf=0.5, weight_stylo=0.5):\n",
    "\n",
    "    # HuggingFace predictions\n",
    "    probs_roberta = hf_model.predict_proba(texts)\n",
    "\n",
    "    # Stylometric predictions\n",
    "    probs_stylo = stylometric_model.predict_proba(texts)\n",
    "\n",
    "    # Ensemble prediction (weighted average)\n",
    "    final_probs = weight_hf * probs_roberta + weight_stylo * probs_stylo\n",
    "    final_preds = (final_probs >= 0.5).astype(int)\n",
    "\n",
    "    return final_preds, final_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4UTC7dIT_0Q"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObEp5bS7qmA3",
    "outputId": "bd26aa3a-adac-4718-f751-4ff5fdd7b0a0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "stylometric_model = StylometricClassifier.load(\"/content/drive/MyDrive/stylometric_model.pkl\")\n",
    "roberta_model = HuggingFaceTransformer.load(\"/content/drive/MyDrive/hf_roberta_new\")\n",
    "\n",
    "preds,_ = predict_ensemble(texts, roberta_model, stylometric_model, weight_hf=0.3, weight_stylo=0.7)\n",
    "\n",
    "# print(preds)\n",
    "\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision = precision_score(labels, preds, pos_label=1)\n",
    "recall = recall_score(labels, preds, pos_label=1)\n",
    "f1 = f1_score(labels, preds, pos_label=1)\n",
    "print(\"\\n\")\n",
    "\n",
    "#change title description based on weight choices and test dataset choice\n",
    "print(\"========== Ensemble Evaluation Summary (Weighted: 0.3 RoBERTa / 0.7 Stylometric) ==========\\n\")\n",
    "#change description to match dataset used\n",
    "print(f\"Dataset: hewlett\")\n",
    "print(f\"Total Samples: {len(labels)}\")\n",
    "print(f\"Human Samples: {len(human_texts)} | AI Samples: {len(machine_texts)}\\n\")\n",
    "\n",
    "print(\"Overall Performance (AI Class = 1):\")\n",
    "print(f\"  Accuracy : {accuracy:.3f}\")\n",
    "print(f\"  Precision: {precision:.3f} (How many predicted AIs were actually AI)\")\n",
    "print(f\"  Recall   : {recall:.3f} (How many actual AIs were correctly detected)\")\n",
    "print(f\"  F1 Score : {f1:.3f} (Harmonic mean of precision and recall)\\n\")\n",
    "\n",
    "print(\"Classification Report (per class):\")\n",
    "print(classification_report(labels, preds, target_names=[\"Human\", \"AI\"]))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
